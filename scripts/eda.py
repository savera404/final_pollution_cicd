############ DATA CLEANING ###################################



"""
AQI Prediction Pipeline - Part 1: Data Preprocessing & US EPA Conversion
=========================================================================
This script handles data cleaning, US EPA AQI conversion (1-500 scale),
and feature engineering for air quality prediction.
"""

import pandas as pd
import numpy as np
from datetime import datetime

# ============================================================================
# STEP 1: US EPA AQI CONVERSION FUNCTIONS (1-500 Scale)
# ============================================================================

def calculate_aqi_single_pollutant(concentration, pollutant):
    """
    Calculate AQI for a single pollutant using US EPA breakpoints (1-500 scale)

    Parameters:
    -----------
    concentration : float
        Pollutant concentration in ¬µg/m¬≥
    pollutant : str
        Name of pollutant ('pm2_5', 'pm10', 'o3', 'no2', 'so2', 'co')

    Returns:
    --------
    float : AQI value (1-500)
    """

    # US EPA AQI Breakpoints [C_low, C_high, AQI_low, AQI_high]
    # Reference: https://www.airnow.gov/aqi/aqi-basics/

    breakpoints = {
        'pm2_5': [  # 24-hour average in ¬µg/m¬≥
            (0.0, 12.0, 0, 50),
            (12.1, 35.4, 51, 100),
            (35.5, 55.4, 101, 150),
            (55.5, 150.4, 151, 200),
            (150.5, 250.4, 201, 300),
            (250.5, 350.4, 301, 400),
            (350.5, 500.4, 401, 500)
        ],
        'pm10': [  # 24-hour average in ¬µg/m¬≥
            (0, 54, 0, 50),
            (55, 154, 51, 100),
            (155, 254, 101, 150),
            (255, 354, 151, 200),
            (355, 424, 201, 300),
            (425, 504, 301, 400),
            (505, 604, 401, 500)
        ],
        'o3': [  # 8-hour average in ppb (need to convert from ¬µg/m¬≥)
            (0, 54, 0, 50),
            (55, 70, 51, 100),
            (71, 85, 101, 150),
            (86, 105, 151, 200),
            (106, 200, 201, 300),
            (201, 404, 301, 400),
            (405, 604, 401, 500)
        ],
        'no2': [  # 1-hour average in ppb
            (0, 53, 0, 50),
            (54, 100, 51, 100),
            (101, 360, 101, 150),
            (361, 649, 151, 200),
            (650, 1249, 201, 300),
            (1250, 1649, 301, 400),
            (1650, 2049, 401, 500)
        ],
        'so2': [  # 1-hour average in ppb
            (0, 35, 0, 50),
            (36, 75, 51, 100),
            (76, 185, 101, 150),
            (186, 304, 151, 200),
            (305, 604, 201, 300),
            (605, 804, 301, 400),
            (805, 1004, 401, 500)
        ],
        'co': [  # 8-hour average in ppm (mg/m¬≥ * 0.873)
            (0.0, 4.4, 0, 50),
            (4.5, 9.4, 51, 100),
            (9.5, 12.4, 101, 150),
            (12.5, 15.4, 151, 200),
            (15.5, 30.4, 201, 300),
            (30.5, 40.4, 301, 400),
            (40.5, 50.4, 401, 500)
        ]
    }

    if pollutant not in breakpoints:
        return np.nan

    # Convert concentrations for specific pollutants
    if pollutant == 'o3':
        # Convert ¬µg/m¬≥ to ppb: ppb = (¬µg/m¬≥ √ó 24.45) / 48
        concentration = (concentration * 24.45) / 48
    elif pollutant == 'no2':
        # Convert ¬µg/m¬≥ to ppb: ppb = (¬µg/m¬≥ √ó 24.45) / 46
        concentration = (concentration * 24.45) / 46
    elif pollutant == 'so2':
        # Convert ¬µg/m¬≥ to ppb: ppb = (¬µg/m¬≥ √ó 24.45) / 64
        concentration = (concentration * 24.45) / 64
    elif pollutant == 'co':
        # Convert ¬µg/m¬≥ to ppm: ppm = (mg/m¬≥) √ó 0.873
        concentration = (concentration / 1000) * 0.873

    # Find appropriate breakpoint
    for c_low, c_high, aqi_low, aqi_high in breakpoints[pollutant]:
        if c_low <= concentration <= c_high:
            # Linear interpolation formula
            aqi = ((aqi_high - aqi_low) / (c_high - c_low)) * (concentration - c_low) + aqi_low
            return round(aqi, 2)

    # If concentration exceeds all breakpoints, return 500 (hazardous)
    return 500.0


def calculate_overall_aqi(row):
    """
    Calculate overall AQI as the maximum of all individual pollutant AQIs

    Parameters:
    -----------
    row : pandas Series
        Row containing pollutant concentrations

    Returns:
    --------
    dict : Dictionary with overall AQI and dominant pollutant
    """
    pollutants = {
        'pm2_5': row.get('pm2_5', np.nan),
        'pm10': row.get('pm10', np.nan),
        'o3': row.get('o3', np.nan),
        'no2': row.get('no2', np.nan),
        'so2': row.get('so2', np.nan),
        'co': row.get('co', np.nan)
    }

    aqi_values = {}
    for pollutant, concentration in pollutants.items():
        if pd.notna(concentration) and concentration >= 0:
            aqi_values[pollutant] = calculate_aqi_single_pollutant(concentration, pollutant)

    if not aqi_values:
        return {'aqi': np.nan, 'dominant_pollutant': None}

    # Overall AQI is the maximum of all pollutant AQIs
    dominant_pollutant = max(aqi_values, key=aqi_values.get)
    overall_aqi = aqi_values[dominant_pollutant]

    return {
        'aqi': overall_aqi,
        'dominant_pollutant': dominant_pollutant,
        **{f'aqi_{k}': v for k, v in aqi_values.items()}
    }

# ============================================================================
# STEP 2: DATA CLEANING & PREPROCESSING
# ============================================================================

def clean_air_quality_data(df):
    """
    Clean and preprocess air quality data

    Steps:
    1. Handle missing values
    2. Remove duplicates
    3. Handle outliers
    4. Validate data ranges
    5. Convert AQI column if present

    Parameters:
    -----------
    df : pandas DataFrame
        Raw air quality data

    Returns:
    --------
    pandas DataFrame : Cleaned data
    """
    print("=" * 70)
    print("STARTING DATA CLEANING PROCESS")
    print("=" * 70)

    df_clean = df.copy()
    initial_rows = len(df_clean)
    print(f"\nüìä Initial dataset size: {initial_rows} rows, {len(df_clean.columns)} columns")

    # 1. Handle datetime
    if 'datetime_utc' in df_clean.columns:
        print("\n‚è∞ Converting datetime column...")
        df_clean['datetime_utc'] = pd.to_datetime(df_clean['datetime_utc'], errors='coerce')
        df_clean = df_clean.dropna(subset=['datetime_utc'])
        print(f"   ‚úì Removed {initial_rows - len(df_clean)} rows with invalid datetime")

    # 2. Remove duplicate timestamps
    if 'datetime_utc' in df_clean.columns:
        before_dedup = len(df_clean)
        df_clean = df_clean.drop_duplicates(subset=['datetime_utc'], keep='first')
        print(f"   ‚úì Removed {before_dedup - len(df_clean)} duplicate timestamps")

    # 3. Handle negative values (pollutants can't be negative)
    pollutant_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']
    print(f"\nüîç Checking for negative values in pollutant columns...")

    for col in pollutant_cols:
        if col in df_clean.columns:
            negative_count = (df_clean[col] < 0).sum()
            if negative_count > 0:
                print(f"   ‚ö†Ô∏è  {col}: Found {negative_count} negative values, setting to 0")
                df_clean[col] = df_clean[col].clip(lower=0)

    # 4. Handle extreme outliers using IQR method (per pollutant)
    print(f"\nüìà Handling extreme outliers (values beyond 3√óIQR)...")
    for col in pollutant_cols:
        if col in df_clean.columns:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR  # Using 3√óIQR for extreme outliers only
            upper_bound = Q3 + 3 * IQR

            outliers = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()
            if outliers > 0:
                print(f"   ‚ö†Ô∏è  {col}: Capping {outliers} extreme outliers")
                df_clean[col] = df_clean[col].clip(lower=max(0, lower_bound), upper=upper_bound)

    # 5. Handle missing values
    print(f"\nüîß Handling missing values...")
    missing_before = df_clean.isnull().sum().sum()

    # For pollutants: Use forward fill then backward fill (reasonable for time series)
    for col in pollutant_cols:
        if col in df_clean.columns:
            missing = df_clean[col].isnull().sum()
            if missing > 0:
                df_clean[col] = df_clean[col].fillna(method='ffill').fillna(method='bfill')
                # If still missing, fill with median
                if df_clean[col].isnull().sum() > 0:
                    df_clean[col] = df_clean[col].fillna(df_clean[col].median())
                print(f"   ‚úì {col}: Filled {missing} missing values")

    missing_after = df_clean.isnull().sum().sum()
    print(f"   ‚úì Total missing values reduced from {missing_before} to {missing_after}")

    # 6. Remove AQI column if present (we'll recalculate it)
    if 'aqi' in df_clean.columns:
        print(f"\nüóëÔ∏è  Removing existing 'aqi' column (will be recalculated)")
        df_clean = df_clean.drop(columns=['aqi'])

    print(f"\n‚úÖ CLEANING COMPLETE")
    print(f"   Final dataset size: {len(df_clean)} rows ({initial_rows - len(df_clean)} rows removed)")
    print("=" * 70)

    return df_clean


# ============================================================================
# STEP 3: FEATURE ENGINEERING
# ============================================================================

def create_features(df):
    """
    Create additional features for better AQI prediction

    Features created:
    - Temporal features (hour, day, month, season)
    - Pollutant ratios and interactions
    - Rolling averages
    - Pollution intensity indicators

    Parameters:
    -----------
    df : pandas DataFrame
        Cleaned data with datetime

    Returns:
    --------
    pandas DataFrame : Data with engineered features
    """
    print("\n" + "=" * 70)
    print("FEATURE ENGINEERING")
    print("=" * 70)

    df_features = df.copy()

    # 1. Temporal features
    if 'datetime_utc' in df_features.columns:
        print("\nüïê Creating temporal features...")
        df_features['hour'] = df_features['datetime_utc'].dt.hour
        df_features['day_of_week'] = df_features['datetime_utc'].dt.dayofweek
        df_features['day_of_month'] = df_features['datetime_utc'].dt.day
        df_features['month'] = df_features['datetime_utc'].dt.month
        df_features['is_weekend'] = (df_features['day_of_week'] >= 5).astype(int)

        # Season (meteorological seasons)
        df_features['season'] = df_features['month'].apply(
            lambda x: 'winter' if x in [12, 1, 2] else
                     'spring' if x in [3, 4, 5] else
                     'summer' if x in [6, 7, 8] else 'fall'
        )

        # Time of day
        df_features['time_of_day'] = df_features['hour'].apply(
            lambda x: 'night' if x < 6 or x >= 22 else
                     'morning' if x < 12 else
                     'afternoon' if x < 18 else 'evening'
        )

        print(f"   ‚úì Created: hour, day_of_week, month, season, time_of_day, is_weekend")

    # 2. Pollutant ratios and interactions
    print("\nüî¨ Creating pollutant interaction features...")

    # PM ratio (indicates particle size distribution)
    if 'pm2_5' in df_features.columns and 'pm10' in df_features.columns:
        df_features['pm_ratio'] = df_features['pm2_5'] / (df_features['pm10'] + 1e-6)
        print(f"   ‚úì pm_ratio: PM2.5/PM10 ratio")

    # Nitrogen oxides ratio
    if 'no2' in df_features.columns and 'no' in df_features.columns:
        df_features['nox_ratio'] = df_features['no2'] / (df_features['no'] + df_features['no2'] + 1e-6)
        print(f"   ‚úì nox_ratio: NO2/(NO+NO2) ratio")

    # Total particulate matter
    if 'pm2_5' in df_features.columns and 'pm10' in df_features.columns:
        df_features['total_pm'] = df_features['pm2_5'] + df_features['pm10']
        print(f"   ‚úì total_pm: Sum of PM2.5 and PM10")

    # Total gaseous pollutants
    gas_cols = ['co', 'no', 'no2', 'o3', 'so2']
    available_gas = [col for col in gas_cols if col in df_features.columns]
    if available_gas:
        df_features['total_gases'] = df_features[available_gas].sum(axis=1)
        print(f"   ‚úì total_gases: Sum of gaseous pollutants")

    # 3. Rolling averages (for temporal patterns)
    print("\nüìä Creating rolling average features (window=3)...")
    pollutant_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10']

    for col in pollutant_cols:
        if col in df_features.columns:
            df_features[f'{col}_rolling_3'] = df_features[col].rolling(window=3, min_periods=1).mean()
            print(f"   ‚úì {col}_rolling_3: 3-period rolling average")

    # 4. Pollution intensity indicators
    print("\n‚ö° Creating pollution intensity indicators...")

    if 'pm2_5' in df_features.columns:
        df_features['pm2_5_high'] = (df_features['pm2_5'] > 35.4).astype(int)  # Above Good level
        print(f"   ‚úì pm2_5_high: Binary indicator (>35.4 ¬µg/m¬≥)")

    if 'pm10' in df_features.columns:
        df_features['pm10_high'] = (df_features['pm10'] > 154).astype(int)  # Above Good level
        print(f"   ‚úì pm10_high: Binary indicator (>154 ¬µg/m¬≥)")

    print("\n‚úÖ FEATURE ENGINEERING COMPLETE")
    print(f"   Total features: {len(df_features.columns)}")
    print("=" * 70)

    return df_features

# ============================================================================
# STEP 4: CALCULATE AQI AND PREPARE FINAL DATASET
# ============================================================================

def prepare_final_dataset(df):
    """
    Calculate AQI values and prepare final dataset for model training

    Parameters:
    -----------
    df : pandas DataFrame
        Data with all features

    Returns:
    --------
    pandas DataFrame : Final dataset with AQI calculated
    """
    print("\n" + "=" * 70)
    print("CALCULATING AQI VALUES")
    print("=" * 70)

    df_final = df.copy()

    # Calculate individual pollutant AQIs and overall AQI
    print("\nüéØ Calculating AQI for all pollutants...")
    aqi_results = df_final.apply(calculate_overall_aqi, axis=1)
    aqi_df = pd.DataFrame(aqi_results.tolist())

    # Merge AQI results
    df_final = pd.concat([df_final, aqi_df], axis=1)

    # Statistics
    print(f"\nüìà AQI Statistics:")
    print(f"   Mean AQI: {df_final['aqi'].mean():.2f}")
    print(f"   Median AQI: {df_final['aqi'].median():.2f}")
    print(f"   Min AQI: {df_final['aqi'].min():.2f}")
    print(f"   Max AQI: {df_final['aqi'].max():.2f}")

    # AQI categories
    df_final['aqi_category'] = pd.cut(
        df_final['aqi'],
        bins=[0, 50, 100, 150, 200, 300, 400, 500],
        labels=['Good', 'Moderate', 'Unhealthy for Sensitive', 'Unhealthy',
                'Very Unhealthy', 'Hazardous', 'Beyond Hazardous']
    )

    print(f"\nüìä AQI Category Distribution:")
    print(df_final['aqi_category'].value_counts().sort_index())

    print("\n‚úÖ AQI CALCULATION COMPLETE")
    print("=" * 70)

    return df_final


# ============================================================================
# MAIN PIPELINE FUNCTION
# ============================================================================

def preprocess_air_quality_pipeline(df):
    """
    Complete preprocessing pipeline for air quality data

    Parameters:
    -----------
    df : pandas DataFrame
        Raw air quality data from OpenWeather API

    Returns:
    --------
    pandas DataFrame : Fully processed data ready for model training
    """
    print("\n" + "="*70)
    print("AIR QUALITY DATA PREPROCESSING PIPELINE")
    print("="*70)
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Step 1: Clean data
    df_clean = clean_air_quality_data(df)

    # Step 2: Feature engineering
    df_features = create_features(df_clean)

    # Step 3: Calculate AQI
    df_final = prepare_final_dataset(df_features)

    print(f"\nüéâ PIPELINE COMPLETE!")
    print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Final dataset shape: {df_final.shape}")
    print("="*70 + "\n")

    return df_final


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Load your data
    # df = pd.read_csv('your_air_quality_data.csv')

    # Or use the sample data from the image

    sample_data = pd.read_csv("data/2_years.csv")
    df = pd.DataFrame(sample_data)

    # Run pipeline
    df_processed = preprocess_air_quality_pipeline(df)

    # Display results
    print("\nSample of processed data:")
    print(df_processed[['datetime_utc', 'aqi', 'aqi_category', 'dominant_pollutant',
                        'pm2_5', 'pm10', 'total_pm']].head(10))

    # Save processed data
    # df_processed.to_csv('processed_air_quality_data.csv', index=False)
    print("\n‚úÖ Data is ready for Hopsworks upload!")




################## EDA AND FEATURE SELECTION ###########################




"""
AQI Prediction Pipeline - Part 2: EDA & Hopsworks Integration
================================================================
This script performs comprehensive EDA and uploads data to Hopsworks Feature Store
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# EXPLORATORY DATA ANALYSIS (EDA)
# ============================================================================

def perform_comprehensive_eda(df, target='aqi'):
    """
    Perform comprehensive EDA to understand data patterns and relationships

    Parameters:
    -----------
    df : pandas DataFrame
        Processed air quality data
    target : str
        Target variable name (default: 'aqi')

    Returns:
    --------
    dict : EDA insights and recommendations
    """
    print("\n" + "="*70)
    print("EXPLORATORY DATA ANALYSIS (EDA)")
    print("="*70)

    insights = {}

    # 1. Basic Statistics
    print("\nüìä 1. BASIC STATISTICS")
    print("-" * 70)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    print(f"Numeric features: {len(numeric_cols)}")
    print(f"\nTarget variable '{target}' statistics:")
    print(df[target].describe())

    insights['numeric_features'] = numeric_cols
    insights['target_stats'] = df[target].describe().to_dict()

    # 2. Distribution Analysis
    print("\nüìà 2. DISTRIBUTION ANALYSIS")
    print("-" * 70)

    # Check skewness
    skewness = {}
    for col in numeric_cols[:10]:  # Check first 10 numeric columns
        skew = df[col].skew()
        skewness[col] = skew
        if abs(skew) > 1:
            print(f"   ‚ö†Ô∏è  {col}: Highly skewed (skewness={skew:.2f})")

    insights['skewness'] = skewness

    # Target variable distribution
    print(f"\n   Target '{target}' distribution:")
    print(f"   - Skewness: {df[target].skew():.2f}")
    print(f"   - Kurtosis: {df[target].kurtosis():.2f}")

    # 3. Missing Values Analysis
    print("\nüîç 3. MISSING VALUES ANALYSIS")
    print("-" * 70)
    missing = df.isnull().sum()
    if missing.sum() == 0:
        print("   ‚úì No missing values found!")
    else:
        print("   Missing values per column:")
        for col in missing[missing > 0].index:
            print(f"   - {col}: {missing[col]} ({missing[col]/len(df)*100:.2f}%)")

    insights['missing_values'] = missing.to_dict()

    # 4. Correlation Analysis
    print("\nüîó 4. CORRELATION ANALYSIS")
    print("-" * 70)

    # Correlation with target
    correlations = df[numeric_cols].corrwith(df[target]).sort_values(ascending=False)

    print(f"\n   Top 10 features correlated with '{target}':")
    top_corr = correlations.head(11)[1:]  # Exclude target itself
    for feat, corr in top_corr.items():
        print(f"   {feat:30s}: {corr:6.3f}")

    insights['top_correlations'] = top_corr.to_dict()

    # Identify multicollinearity
    print("\n   üî¥ Checking for multicollinearity (correlation > 0.9):")
    corr_matrix = df[numeric_cols].corr()
    high_corr_pairs = []

    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.9:
                high_corr_pairs.append({
                    'feature1': corr_matrix.columns[i],
                    'feature2': corr_matrix.columns[j],
                    'correlation': corr_matrix.iloc[i, j]
                })

    if high_corr_pairs:
        for pair in high_corr_pairs:
            print(f"   ‚ö†Ô∏è  {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}")
        print(f"\n   üí° Recommendation: Consider removing one feature from highly correlated pairs")
    else:
        print("   ‚úì No severe multicollinearity detected")

    insights['high_correlation_pairs'] = high_corr_pairs

    # 5. Outlier Detection
    print("\nüìâ 5. OUTLIER DETECTION (IQR Method)")
    print("-" * 70)

    outlier_counts = {}
    for col in [target] + ['pm2_5', 'pm10', 'co', 'no2', 'o3', 'so2']:
        if col in df.columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
            outlier_counts[col] = outliers
            if outliers > 0:
                print(f"   {col:15s}: {outliers:4d} outliers ({outliers/len(df)*100:.1f}%)")

    insights['outlier_counts'] = outlier_counts

    # 6. Feature Importance (Statistical Tests)
    print("\n‚≠ê 6. FEATURE IMPORTANCE ANALYSIS")
    print("-" * 70)

    # Prepare features for analysis (exclude non-numeric and target)
    feature_cols = [col for col in numeric_cols if col != target and col not in
                    ['aqi_pm2_5', 'aqi_pm10', 'aqi_co', 'aqi_no2', 'aqi_o3', 'aqi_so2']]

    X = df[feature_cols].fillna(0)
    y = df[target]

    # F-statistic based selection
    print("\n   üìä F-statistic based feature importance:")
    selector_f = SelectKBest(score_func=f_regression, k='all')
    selector_f.fit(X, y)

    f_scores = pd.DataFrame({
        'feature': feature_cols,
        'f_score': selector_f.scores_
    }).sort_values('f_score', ascending=False)

    print("\n   Top 15 features by F-score:")
    for idx, row in f_scores.head(15).iterrows():
        print(f"   {row['feature']:30s}: {row['f_score']:10.2f}")

    # Mutual Information based selection
    print("\n   üîç Mutual Information based feature importance:")
    mi_scores = mutual_info_regression(X, y, random_state=42)

    mi_scores_df = pd.DataFrame({
        'feature': feature_cols,
        'mi_score': mi_scores
    }).sort_values('mi_score', ascending=False)

    print("\n   Top 15 features by Mutual Information:")
    for idx, row in mi_scores_df.head(15).iterrows():
        print(f"   {row['feature']:30s}: {row['mi_score']:10.4f}")

    insights['f_scores'] = f_scores.to_dict('records')
    insights['mi_scores'] = mi_scores_df.to_dict('records')

    print("\n‚úÖ EDA COMPLETE")
    print("="*70)

    return insights


# ============================================================================
# FEATURE SELECTION
# ============================================================================

def select_best_features(df, target='aqi', n_features=20):
    """
    Select the best features for model training using multiple methods

    Parameters:
    -----------
    df : pandas DataFrame
        Processed data with all features
    target : str
        Target variable name
    n_features : int
        Number of top features to select

    Returns:
    --------
    list : Selected feature names
    """
    print("\n" + "="*70)
    print("FEATURE SELECTION")
    print("="*70)

    # Get numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # Exclude target and individual pollutant AQI columns
    exclude_cols = [target, 'aqi_pm2_5', 'aqi_pm10', 'aqi_co', 'aqi_no2', 'aqi_o3', 'aqi_so2']
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]

    print(f"\nüìä Total available features: {len(feature_cols)}")
    print(f"üéØ Selecting top {n_features} features...\n")

    X = df[feature_cols].fillna(0)
    y = df[target]

    # Method 1: F-statistic
    selector_f = SelectKBest(score_func=f_regression, k=min(n_features, len(feature_cols)))
    selector_f.fit(X, y)
    f_features = [feature_cols[i] for i in selector_f.get_support(indices=True)]

    # Method 2: Mutual Information
    mi_scores = mutual_info_regression(X, y, random_state=42)
    mi_features = pd.Series(mi_scores, index=feature_cols).nlargest(n_features).index.tolist()

    # Method 3: Correlation
    correlations = X.corrwith(y).abs().nlargest(n_features).index.tolist()

    # Combine all methods (union of top features)
    selected_features = list(set(f_features + mi_features + correlations))

    print("üîπ Feature Selection Methods:")
    print(f"   ‚Ä¢ F-statistic top features: {len(f_features)}")
    print(f"   ‚Ä¢ Mutual Information top features: {len(mi_features)}")
    print(f"   ‚Ä¢ Correlation top features: {len(correlations)}")
    print(f"\n‚úÖ Final selected features: {len(selected_features)}")

    # Ensure critical pollutant features are included
    critical_features = ['pm2_5', 'pm10', 'no2', 'co', 'o3', 'so2']
    for feat in critical_features:
        if feat in df.columns and feat not in selected_features:
            selected_features.append(feat)
            print(f"   ‚ûï Added critical feature: {feat}")

    print(f"\nüìã Final feature list ({len(selected_features)} features):")
    for i, feat in enumerate(sorted(selected_features), 1):
        print(f"   {i:2d}. {feat}")

    print("\n" + "="*70)

    return selected_features


# ============================================================================
# DATA VALIDATION FOR HOPSWORKS
# ============================================================================

def validate_dataframe_for_hopsworks(df):
    """
    Validate and prepare DataFrame for Hopsworks upload

    This function checks for common issues that cause Hopsworks upload failures
    and fixes them automatically.

    Parameters:
    -----------
    df : pandas DataFrame
        DataFrame to validate and prepare

    Returns:
    --------
    pandas DataFrame : Validated and prepared DataFrame
    """
    print("\n" + "="*70)
    print("VALIDATING DATAFRAME FOR HOPSWORKS")
    print("="*70)

    df_clean = df.copy()
    issues_fixed = []

    # 1. Check for MultiIndex
    if isinstance(df_clean.index, pd.MultiIndex):
        print("‚ö†Ô∏è  Issue: MultiIndex detected")
        df_clean = df_clean.reset_index(drop=True)
        issues_fixed.append("Reset MultiIndex")

    # 2. Check for MultiIndex columns
    if isinstance(df_clean.columns, pd.MultiIndex):
        print("‚ö†Ô∏è  Issue: MultiIndex columns detected")
        df_clean.columns = ['_'.join(map(str, col)).strip() for col in df_clean.columns.values]
        issues_fixed.append("Flattened MultiIndex columns")

    # 3. Check for duplicate column names
    if df_clean.columns.duplicated().any():
        print("‚ö†Ô∏è  Issue: Duplicate column names detected")
        df_clean = df_clean.loc[:, ~df_clean.columns.duplicated()]
        issues_fixed.append("Removed duplicate columns")

    # 4. Check for mixed types in columns
    print("\nüîç Checking column data types...")
    for col in df_clean.columns:
        # Get unique types in column
        types_in_col = df_clean[col].apply(type).unique()
        if len(types_in_col) > 2:  # More than 2 types (including NaN)
            print(f"‚ö†Ô∏è  Issue: Mixed types in column '{col}': {types_in_col}")
            # Try to convert to most common type
            if df_clean[col].dtype == 'object':
                try:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                    issues_fixed.append(f"Converted '{col}' to numeric")
                except:
                    df_clean[col] = df_clean[col].astype(str)
                    issues_fixed.append(f"Converted '{col}' to string")

    # 5. Handle datetime columns
    datetime_cols = df_clean.select_dtypes(include=['datetime64']).columns
    if len(datetime_cols) > 0:
        print(f"\nüìÖ Processing {len(datetime_cols)} datetime columns...")
        for col in datetime_cols:
            # Convert to Unix timestamp (milliseconds since epoch)
            df_clean[col] = df_clean[col].astype('int64') // 10**6
            issues_fixed.append(f"Converted '{col}' to timestamp")

    # 6. Handle categorical columns
    categorical_cols = df_clean.select_dtypes(include=['category']).columns
    if len(categorical_cols) > 0:
        print(f"\nüè∑Ô∏è  Processing {len(categorical_cols)} categorical columns...")
        for col in categorical_cols:
            df_clean[col] = df_clean[col].astype(str)
            issues_fixed.append(f"Converted '{col}' category to string")

    # 7. Handle object columns (ensure they're all strings)
    object_cols = df_clean.select_dtypes(include=['object']).columns
    if len(object_cols) > 0:
        print(f"\nüìù Processing {len(object_cols)} object columns...")
        for col in object_cols:
            df_clean[col] = df_clean[col].fillna('unknown').astype(str)

    # 8. Ensure numeric columns are float64 or int64
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df_clean[col].dtype not in ['int64', 'float64']:
            if df_clean[col].apply(lambda x: x == int(x) if pd.notna(x) else True).all():
                df_clean[col] = df_clean[col].astype('int64')
            else:
                df_clean[col] = df_clean[col].astype('float64')

    # 9. Handle NaN values
    nan_count = df_clean.isnull().sum().sum()
    if nan_count > 0:
        print(f"\n‚ö†Ô∏è  Found {nan_count} NaN values, filling with defaults...")
        for col in df_clean.columns:
            if df_clean[col].dtype in ['float64', 'int64']:
                df_clean[col] = df_clean[col].fillna(0)
            else:
                df_clean[col] = df_clean[col].fillna('unknown')
        issues_fixed.append("Filled NaN values")

    # 10. Check for infinity values
    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        inf_count = np.isinf(df_clean[col]).sum()
        if inf_count > 0:
            print(f"‚ö†Ô∏è  Found {inf_count} infinity values in '{col}'")
            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan).fillna(0)
            issues_fixed.append(f"Replaced infinity in '{col}'")

    # 11. Validate column names (no special characters except underscore)
    invalid_cols = [col for col in df_clean.columns if not col.replace('_', '').isalnum()]
    if invalid_cols:
        print(f"\n‚ö†Ô∏è  Invalid column names detected: {invalid_cols}")
        for col in invalid_cols:
            new_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in col)
            df_clean = df_clean.rename(columns={col: new_col})
        issues_fixed.append("Sanitized column names")

    # Final validation
    print(f"\n‚úÖ VALIDATION COMPLETE")
    if issues_fixed:
        print(f"\nüîß Issues fixed ({len(issues_fixed)}):")
        for i, issue in enumerate(issues_fixed, 1):
            print(f"   {i}. {issue}")
    else:
        print("   No issues found - DataFrame is ready!")

    print(f"\nüìä Final DataFrame Summary:")
    print(f"   ‚Ä¢ Shape: {df_clean.shape}")
    print(f"   ‚Ä¢ Columns: {len(df_clean.columns)}")
    print(f"   ‚Ä¢ Data types:")
    dtype_counts = df_clean.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"      - {dtype}: {count} columns")
    print(f"   ‚Ä¢ Missing values: {df_clean.isnull().sum().sum()}")
    print(f"   ‚Ä¢ Duplicate rows: {df_clean.duplicated().sum()}")

    print("="*70)

    return df_clean


# Update the upload function to use validation
def upload_to_hopsworks(df, feature_group_name="karachi_air_quality_features", version=1):
    """
    Upload processed data to Hopsworks Feature Store

    Parameters:
    -----------
    df : pandas DataFrame
        Processed data with all features and AQI
    feature_group_name : str
        Name for the feature group
    version : int
        Version number

    Returns:
    --------
    feature_group : Hopsworks feature group object
    """
    print("\n" + "="*70)
    print("UPLOADING TO HOPSWORKS FEATURE STORE")
    print("="*70)

    try:
        import hopsworks
        import os

        print("\nüîê Connecting to Hopsworks...")

        # # Login to Hopsworks
        # project = hopsworks.login(api_key_value=os.getenv("HOPSWORKS_API_KEY"))
        project = hopsworks.login(
            project="pollution_cicd",
            api_key_value=os.getenv("HOPSWORKS_API_KEY")
         )
        print(f"   ‚úì Connected to project: {project.name}")

        # Get feature store
        fs = project.get_feature_store()
        print(f"   ‚úì Accessed feature store")

        # CRITICAL: Validate and prepare dataframe
        print(f"\nüîç Validating DataFrame before upload...")
        df_upload = validate_dataframe_for_hopsworks(df)

        # Add unique ID if not present
        if 'id' not in df_upload.columns:
            df_upload.insert(0, 'id', range(1, len(df_upload) + 1))
            print(f"   ‚úì Added unique ID column")

        # Convert datetime to timestamp (int64) for better compatibility
        if 'datetime_utc' in df_upload.columns:
            # Convert to pandas datetime first if not already
            df_upload['datetime_utc'] = pd.to_datetime(df_upload['datetime_utc'], errors='coerce')
            # Convert to Unix timestamp (milliseconds)
            df_upload['datetime_utc'] = df_upload['datetime_utc'].astype('int64') // 10**6
            print(f"   ‚úì Converted datetime to timestamp")

        # Handle categorical columns - convert to string explicitly
        categorical_cols = df_upload.select_dtypes(include=['object', 'category']).columns.tolist()
        for col in categorical_cols:
            df_upload[col] = df_upload[col].fillna('unknown').astype(str)
            print(f"   ‚úì Converted categorical column: {col}")

        # Ensure all numeric columns are proper numeric types
        numeric_cols = df_upload.select_dtypes(include=[np.number]).columns.tolist()
        for col in numeric_cols:
            if col != 'id' and col != 'datetime_utc':
                # Convert to float64 to avoid dtype issues
                df_upload[col] = pd.to_numeric(df_upload[col], errors='coerce').astype('float64')

        # Fill any remaining NaN values
        df_upload = df_upload.fillna(0)

        # Verify dataframe integrity
        print(f"\nüîç Verifying data integrity...")
        print(f"   ‚Ä¢ Data types check: {df_upload.dtypes.nunique()} unique types")
        print(f"   ‚Ä¢ Missing values: {df_upload.isnull().sum().sum()}")
        print(f"   ‚Ä¢ Duplicate rows: {df_upload.duplicated().sum()}")

        # Remove duplicates if any
        if df_upload.duplicated().sum() > 0:
            df_upload = df_upload.drop_duplicates()
            print(f"   ‚úì Removed duplicate rows")

        print(f"\nüì§ Uploading data...")
        print(f"   ‚Ä¢ Feature group name: {feature_group_name}")
        print(f"   ‚Ä¢ Version: {version}")
        print(f"   ‚Ä¢ Rows: {len(df_upload)}")
        print(f"   ‚Ä¢ Columns: {len(df_upload.columns)}")
        print(f"   ‚Ä¢ Columns: {', '.join(df_upload.columns.tolist()[:10])}...")

        # Create or get feature group (without event_time if datetime was converted)
        fg_params = {
            "name": feature_group_name,
            "version": version,
            "primary_key": ["datetime_utc"],
            "description": "Air Quality data with calculated AQI (US EPA 1-500 scale) and engineered features",
            "online_enabled": False
        }

        # Only add event_time if datetime column exists
        if 'datetime_utc' in df_upload.columns:
            fg_params["event_time"] = "datetime_utc"

        feature_group = fs.get_or_create_feature_group(**fg_params)

        # Insert data with proper options
        print(f"\n‚è≥ Inserting data (this may take a few moments)...")
        feature_group.insert(df_upload, write_options={"wait_for_job": False})

        print(f"\n‚úÖ DATA SUCCESSFULLY UPLOADED TO HOPSWORKS!")
        print(f"   Feature group: {feature_group_name} (v{version})")
        print(f"   Note: Data ingestion job is running in background")
        print("="*70)

        return feature_group

    except ImportError:
        print("\n‚ùå ERROR: hopsworks package not installed")
        print("   Install with: pip install hopsworks")
        return None

    except Exception as e:
        import traceback
        print(f"\n‚ùå ERROR uploading to Hopsworks: {str(e)}")
        print("\nüîç Detailed error traceback:")
        print(traceback.format_exc())
        print("\nüí° Troubleshooting tips:")
        print("   1. Check if your API key is valid")
        print("   2. Ensure all column dtypes are compatible (int, float, str)")
        print("   3. Check for special characters in column names")
        print("   4. Verify no mixed-type columns exist")
        return None


def fetch_from_hopsworks(feature_group_name="karachi_air_quality_features", version=1):
    """
    Fetch data from Hopsworks Feature Store for model training

    Parameters:
    -----------
    feature_group_name : str
        Name of the feature group
    version : int
        Version number

    Returns:
    --------
    pandas DataFrame : Retrieved data
    """
    print("\n" + "="*70)
    print("FETCHING DATA FROM HOPSWORKS FEATURE STORE")
    print("="*70)

    try:
        import hopsworks
        import os

        print("\nüîê Connecting to Hopsworks...")

        # Login to Hopsworks
        # project = hopsworks.login(api_key_value=os.getenv("HOPSWORKS_API_KEY"))
        project = hopsworks.login(
            project="pollution_cicd",
            api_key_value=os.getenv("HOPSWORKS_API_KEY")
         )
        print(f"   ‚úì Connected to project: {project.name}")

        # Get feature store
        fs = project.get_feature_store()

        print(f"\nüì• Fetching feature group...")
        print(f"   ‚Ä¢ Feature group name: {feature_group_name}")
        print(f"   ‚Ä¢ Version: {version}")

        # Get feature group
        feature_group = fs.get_feature_group(
            name=feature_group_name,
            version=version
        )

        # Read data
        df = feature_group.read()

        print(f"\n‚úÖ DATA SUCCESSFULLY RETRIEVED!")
        print(f"   ‚Ä¢ Rows: {len(df)}")
        print(f"   ‚Ä¢ Columns: {len(df.columns)}")
        print("="*70)

        return df

    except ImportError:
        print("\n‚ùå ERROR: hopsworks package not installed")
        print("   Install with: pip install hopsworks")
        return None

    except Exception as e:
        print(f"\n‚ùå ERROR fetching from Hopsworks: {str(e)}")
        return None


# ============================================================================
# VISUALIZATION FUNCTIONS (Optional but helpful)
# ============================================================================

def create_eda_visualizations(df, target='aqi', save_plots=False):
    """
    Create comprehensive EDA visualizations

    Parameters:
    -----------
    df : pandas DataFrame
        Processed data
    target : str
        Target variable name
    save_plots : bool
        Whether to save plots to files
    """
    print("\nüìä Creating EDA visualizations...")

    # Set style
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (15, 10)

    # 1. Target Distribution
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Histogram
    axes[0, 0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)
    axes[0, 0].set_title(f'{target.upper()} Distribution', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('AQI Value')
    axes[0, 0].set_ylabel('Frequency')

    # Box plot
    axes[0, 1].boxplot(df[target])
    axes[0, 1].set_title(f'{target.upper()} Box Plot', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('AQI Value')

    # Q-Q plot
    stats.probplot(df[target], dist="norm", plot=axes[1, 0])
    axes[1, 0].set_title('Q-Q Plot', fontsize=14, fontweight='bold')

    # AQI Category Distribution
    if 'aqi_category' in df.columns:
        df['aqi_category'].value_counts().sort_index().plot(kind='bar', ax=axes[1, 1])
        axes[1, 1].set_title('AQI Category Distribution', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Category')
        axes[1, 1].set_ylabel('Count')
        axes[1, 1].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    if save_plots:
        plt.savefig('eda_target_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 2. Correlation Heatmap
    numeric_cols = df.select_dtypes(include=[np.number]).columns[:15]  # Top 15 features
    plt.figure(figsize=(12, 10))
    sns.heatmap(df[numeric_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm',
                center=0, square=True, linewidths=1)
    plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')
    plt.tight_layout()
    if save_plots:
        plt.savefig('eda_correlation_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("   ‚úì Visualizations created!")


# ============================================================================
# MAIN WORKFLOW
# ============================================================================

def complete_eda_and_upload_pipeline(df_processed, selected_features_count=20):
    """
    Complete pipeline: EDA ‚Üí Feature Selection ‚Üí Hopsworks Upload

    Parameters:
    -----------
    df_processed : pandas DataFrame
        Processed data from Part 1
    selected_features_count : int
        Number of features to select

    Returns:
    --------
    tuple : (selected_features, feature_group)
    """
    # Step 1: Comprehensive EDA
    insights = perform_comprehensive_eda(df_processed, target='aqi')

    # Step 2: Feature Selection
    selected_features = select_best_features(
        df_processed,
        target='aqi',
        n_features=selected_features_count
    )

    # Step 3: Prepare final dataset with selected features
    final_cols = ['id'] if 'id' in df_processed.columns else []
    final_cols += ['datetime_utc'] if 'datetime_utc' in df_processed.columns else []
    final_cols += selected_features + ['aqi', 'aqi_category', 'dominant_pollutant']

    df_final = df_processed[final_cols].copy()

    print(f"\nüì¶ Final dataset prepared with {len(df_final.columns)} columns")

    # Step 4: Upload to Hopsworks
    feature_group = upload_to_hopsworks(
        df_final,
        feature_group_name="air_quality_features",
        version=1
    )

    return selected_features, feature_group


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Assuming you have df_processed from Part 1
    # df_processed = preprocess_air_quality_pipeline(df_processed)

    # Run complete EDA and upload pipeline
    selected_features, feature_group = complete_eda_and_upload_pipeline(
        df_processed,
        selected_features_count=20
    )

    print("\n" + "="*70)
    print("PART 2 COMPLETE - Ready for Model Training!")
    print("="*70)
    print("\nNext steps:")
    print("1. ‚úÖ Data preprocessed and cleaned")
    print("2. ‚úÖ Comprehensive EDA performed")
    print("3. ‚úÖ Best features selected")
    print("4. ‚úÖ Data uploaded to Hopsworks Feature Store")
    print("5. ‚è≠Ô∏è  Ready for Part 3: Model Training & Deployment")

    print("="*70)


